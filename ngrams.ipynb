{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3630d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv, re, math, unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = Path(\"csv-datasets\")\n",
    "LOWERCASE = True\n",
    "LAMBDAS = (0.6, 0.3, 0.1)\n",
    "MIN_FREQ = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5463ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines base characters and combining marks into single characters\n",
    "def nfc(s): \n",
    "    return unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "# replaces multiple whitespaces char with a single space and removes leading and trailing whitespaces\n",
    "def collapse_ws(s): \n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "SPLIT_EN = re.compile(r\"(?<=[.!?])\\s+\")  # after . ! ?\n",
    "SPLIT_HI = re.compile(r\"(?:।|\\?|!)\\s+\")  # Hindi danda ? !\n",
    "\n",
    "# split into sentences (EN: . ? !, HI: । ? !)\n",
    "def sent_split(text, lang):\n",
    "    rx = SPLIT_EN if lang == \"en\" else SPLIT_HI\n",
    "    return [s for s in rx.split(text) if s.strip()]\n",
    "\n",
    "tok_en = RegexpTokenizer(r\"[A-Za-z0-9]+'[A-Za-z0-9]+|[\\w]+\")\n",
    "tok_hi = RegexpTokenizer(r\"[\\w]+\") \n",
    "\n",
    "# tokenize each sentence\n",
    "def word_tokens(text, lang):\n",
    "    return (tok_en.tokenize(text) if lang == \"en\" else tok_hi.tokenize(text))\n",
    "\n",
    "def normalize_text(text, lowercase):\n",
    "    t = collapse_ws(nfc(text))\n",
    "    return t.lower() if lowercase else t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d14567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(csv_path) :\n",
    "    texts = []\n",
    "    with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for r in rdr:\n",
    "            texts.append(r[\"Text\"])\n",
    "    return texts\n",
    "\n",
    "def make_lm_sentences(texts, lang, lowercase):\n",
    "    out = []\n",
    "    for txt in texts:\n",
    "        t = normalize_text(txt, lowercase)\n",
    "        for s in sent_split(t, lang):\n",
    "            toks = word_tokens(s, lang)\n",
    "            if toks:\n",
    "                out.append([\"<s>\", *toks, \"</s>\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42029fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, min_freq):\n",
    "    cnt = Counter()\n",
    "    for s in sentences:\n",
    "        cnt.update(s)\n",
    "    tok2Id = {\"<unk>\": 0, \"<s>\": 1, \"</s>\": 2}\n",
    "    for tok, c in cnt.most_common():\n",
    "        if tok in tok2Id:\n",
    "            continue\n",
    "        if c >= min_freq:\n",
    "            tok2Id[tok] = len(tok2Id)\n",
    "    return tok2Id\n",
    "\n",
    "def map_to_ids(sentences, tok2Id):\n",
    "    uid = tok2Id[\"<unk>\"]\n",
    "    return [[tok2Id.get(t, uid) for t in s] for s in sentences]\n",
    "\n",
    "def count_ngrams(sent_ids):\n",
    "    C1, C2, C3 = Counter(), Counter(), Counter()\n",
    "    T = 0\n",
    "    for seq in sent_ids:\n",
    "        T += len(seq)\n",
    "        for w in seq: # unigrams\n",
    "            C1[w] += 1\n",
    "        for i in range(1, len(seq)): # bigrams\n",
    "            C2[(seq[i-1], seq[i])] += 1\n",
    "        for i in range(2, len(seq)): # trigrams\n",
    "            C3[(seq[i-2], seq[i-1], seq[i])] += 1\n",
    "    return C1, C2, C3, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dcf4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pml_uni(C1, T, w):\n",
    "    return C1.get(w, 0) / T if T > 0 else 0.0\n",
    "\n",
    "def pml_bi(C1, C2, v, w):\n",
    "    den = C1.get(v, 0)\n",
    "    return 0.0 if den == 0 else C2.get((v, w), 0) / den\n",
    "\n",
    "def pml_tri(C2, C3, u, v, w):\n",
    "    den = C2.get((u, v), 0)\n",
    "    return 0.0 if den == 0 else C3.get((u, v, w), 0) / den\n",
    "\n",
    "def logp(C1, C2, C3, T, w, u, v, lambdas):\n",
    "    l3, l2, l1 = lambdas\n",
    "    p3 = pml_tri(C2, C3, u, v, w) if (u is not None and v is not None) else 0.0\n",
    "    p2 = pml_bi(C1, C2, v, w) if (v is not None) else 0.0\n",
    "    p1 = pml_uni(C1, T, w)\n",
    "    p = l3*p3 + l2*p2 + l1*p1\n",
    "    return math.log(p if p > 0.0 else 1e-12)\n",
    "\n",
    "def perplexity(C1, C2, C3, T, test_ids, lambdas):\n",
    "    total_logp = 0.0\n",
    "    total_tok = 0\n",
    "    for seq in test_ids:\n",
    "        for i, w in enumerate(seq):\n",
    "            u = seq[i-2] if i >= 2 else None\n",
    "            v = seq[i-1] if i >= 1 else None\n",
    "            total_logp += logp(C1, C2, C3, T, w, u, v, lambdas)\n",
    "            total_tok  += 1\n",
    "    ppl = math.exp(- total_logp / max(1, total_tok))\n",
    "    return {\"tokens\": total_tok, \"ppl\": ppl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d13e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_path(lang, size):\n",
    "    sub = \"english\" if lang == \"en\" else \"hindi\"\n",
    "    return DATA_DIR / sub / f\"{sub}_{size}.csv\"\n",
    "\n",
    "def run_size(lang, size, lowercase = LOWERCASE, min_freq = MIN_FREQ, lambdas = LAMBDAS):\n",
    "    assert abs(sum(lambdas) - 1.0) < 1e-9, \"λ must sum to 1\"\n",
    "    train_csv = csv_path(lang, size)\n",
    "    test_csv  = csv_path(lang, \"test\")\n",
    "\n",
    "    # 1) Read CSVs\n",
    "    train_texts = read_texts(train_csv)\n",
    "    test_texts  = read_texts(test_csv)\n",
    "\n",
    "    # 2) Build LM sentences\n",
    "    train_sents = make_lm_sentences(train_texts, lang, lowercase)\n",
    "    test_sents  = make_lm_sentences(test_texts,  lang, lowercase)\n",
    "\n",
    "    # 3) Vocab from train\n",
    "    tok2id = build_vocab(train_sents, min_freq=min_freq)\n",
    "    for sp in [\"<unk>\", \"<s>\", \"</s>\"]:\n",
    "        if sp not in tok2id: tok2id[sp] = len(tok2id)\n",
    "\n",
    "    # 4) Map to ids\n",
    "    train_ids = map_to_ids(train_sents, tok2id)\n",
    "    test_ids  = map_to_ids(test_sents,  tok2id)\n",
    "\n",
    "    # 5) Count n-grams\n",
    "    C1, C2, C3, T = count_ngrams(train_ids)\n",
    "\n",
    "    # 6) Perplexity on test with simple interpolation\n",
    "    ppl_info = perplexity(C1, C2, C3, T, test_ids, lambdas)\n",
    "\n",
    "    print(f\"[{\"English\" if lang == 'en' else \"Hindi\"}-{size}] \"\n",
    "          f\"V={len(tok2id)} \"\n",
    "          f\"TrainTok={T} \"\n",
    "          f\"TestTok={ppl_info['tokens']} \"\n",
    "          f\"Perplexity={ppl_info['ppl']:.3f}\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"lang\": lang, \"size\": size,\n",
    "        \"vocab_size\": len(tok2id),\n",
    "        \"train_tokens\": T,\n",
    "        \"test_tokens\": ppl_info[\"tokens\"],\n",
    "        \"ppl\": ppl_info[\"ppl\"],\n",
    "        \"lambdas\": lambdas,\n",
    "        \"min_freq\": min_freq,\n",
    "        \"lowercase\": lowercase\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbf051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Evaluation:\n",
      "\n",
      "[English-2500] V=13920 TrainTok=730301 TestTok=3315101 Perplexity=460.558\n",
      "[English-15000] V=46472 TrainTok=6681508 TestTok=3315101 Perplexity=26.823\n",
      "[English-30000] V=68715 TrainTok=12469632 TestTok=3315101 Perplexity=31.740\n",
      "\n",
      "[Hindi-2500] V=1533 TrainTok=298135 TestTok=3568123 Perplexity=26.138\n",
      "[Hindi-15000] V=2840 TrainTok=1792596 TestTok=3568123 Perplexity=23.818\n",
      "[Hindi-30000] V=3216 TrainTok=3584631 TestTok=3568123 Perplexity=20.155\n"
     ]
    }
   ],
   "source": [
    "def run_language_all_sizes(lang: str):\n",
    "    results = []\n",
    "    for size in [\"2500\", \"15000\", \"30000\"]:\n",
    "        results.append(run_size(lang, size))\n",
    "    return results\n",
    "\n",
    "print(\"Perplexity Evaluation:\\n\")\n",
    "en_results = run_language_all_sizes(\"en\")\n",
    "print()\n",
    "hi_results = run_language_all_sizes(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60012f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classification\n",
    "\n",
    "def read_rows_single_label(csv_file: Path):\n",
    "    rows = []\n",
    "    with Path(csv_file).open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        assert rdr.fieldnames, f\"No headers in {csv_file}\"\n",
    "        k_cat = rdr.fieldnames[0]\n",
    "        k_txt = rdr.fieldnames[1]\n",
    "\n",
    "        for r in rdr:\n",
    "            lab = (r.get(k_cat) or \"\").strip().lower()\n",
    "            txt = r.get(k_txt) or \"\"\n",
    "            if lab:\n",
    "                rows.append((lab, txt))\n",
    "    return rows\n",
    "\n",
    "def doc_tokens(text: str, lang: str, lowercase: bool=True):\n",
    "    t = normalize_text(text, lowercase)\n",
    "    return word_tokens(t, lang)\n",
    "\n",
    "# Unigram vocab (by document frequency) & featurizer\n",
    "def build_unigram_vocab(train_docs, min_df=1):\n",
    "    df = Counter()\n",
    "    for toks in train_docs:\n",
    "        for t in set(toks):  \n",
    "            df[t] += 1\n",
    "    feat2id = {}\n",
    "    for t, c in sorted(df.items(), key=lambda x: (-x[1], x[0])):  # stable ids\n",
    "        if c >= min_df:\n",
    "            feat2id[t] = len(feat2id)\n",
    "    return feat2id\n",
    "\n",
    "def featurize_unigrams(docs, feat2id):\n",
    "    X = []\n",
    "    for toks in docs:\n",
    "        dd = defaultdict(int)\n",
    "        for t in toks:\n",
    "            i = feat2id.get(t)\n",
    "            if i is not None: \n",
    "                dd[i] += 1\n",
    "        X.append(dd)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51cf36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nb(X_train, y_train, C, F):\n",
    "    N = len(X_train)\n",
    "\n",
    "    class_docs = np.bincount(np.array(y_train, dtype=np.int64), minlength=C)\n",
    "    logpi = np.log((class_docs) / (N))\n",
    "\n",
    "    counts = np.zeros((C, F), dtype=np.int64)\n",
    "    totals = np.zeros(C, dtype=np.int64)\n",
    "    for x, c in zip(X_train, y_train):\n",
    "        s = sum(x.values())\n",
    "        totals[c] += s\n",
    "        for f, v in x.items():\n",
    "            counts[c, f] += v\n",
    "    deno = totals[:, None] + F\n",
    "    logtheta = np.log((counts + 1) / np.maximum(deno, 1))\n",
    "    return logpi, logtheta\n",
    "\n",
    "def predict_nb(X, logpi, logtheta):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        s = logpi.copy()\n",
    "        if x:\n",
    "            idx = np.fromiter(x.keys(), dtype=np.int64, count=len(x))\n",
    "            val = np.fromiter(x.values(), dtype=np.float64, count=len(x))\n",
    "            s += (logtheta[:, idx] @ val)\n",
    "        preds.append(int(np.argmax(s)))\n",
    "    return preds\n",
    "\n",
    "def metrics(y_true, y_pred, C):\n",
    "    N = len(y_true)\n",
    "    acc = sum(int(t==p) for t,p in zip(y_true,y_pred)) / max(1, N)\n",
    "\n",
    "    tp = [0]*C; fp = [0]*C; fn = [0]*C\n",
    "    for t,p in zip(y_true,y_pred):\n",
    "        if t == p: tp[t] += 1\n",
    "        else:      fp[p] += 1; fn[t] += 1\n",
    "\n",
    "    precs, recs, f1s = [], [], []\n",
    "    for c in range(C):\n",
    "        P = tp[c] / (tp[c] + fp[c]) if (tp[c] + fp[c]) else 0.0\n",
    "        R = tp[c] / (tp[c] + fn[c]) if (tp[c] + fn[c]) else 0.0\n",
    "        F = 0.0 if (P+R)==0 else 2*P*R/(P+R)\n",
    "        precs.append(P); recs.append(R); f1s.append(F)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": float(np.mean(precs)),\n",
    "        \"recall\":    float(np.mean(recs)),\n",
    "        \"f1\":        float(np.mean(f1s)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9427e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification using Naive Bayes:\n",
      "\n",
      "[English-2500] Acc=0.314  Prec=0.010  Rec=0.012  F1=0.009\n",
      "[English-15000] Acc=0.710  Prec=0.054  Rec=0.036  F1=0.037\n",
      "[English-30000] Acc=0.679  Prec=0.010  Rec=0.007  F1=0.007\n",
      "\n",
      "[Hindi-2500] Acc=0.587  Prec=0.249  Rec=0.144  F1=0.144\n",
      "[Hindi-15000] Acc=0.694  Prec=0.558  Rec=0.350  F1=0.392\n",
      "[Hindi-30000] Acc=0.744  Prec=0.632  Rec=0.485  F1=0.515\n"
     ]
    }
   ],
   "source": [
    "def run_nb(lang, size, lowercase=True, min_df=1):\n",
    "    # 1) Load rows → (label, text)\n",
    "    train_pairs = read_rows_single_label(csv_path(lang, size))\n",
    "    test_pairs  = read_rows_single_label(csv_path(lang, \"test\"))\n",
    "\n",
    "    # 2) Label space from TRAIN only\n",
    "    labels = sorted({lab for lab,_ in train_pairs})\n",
    "    lab2idx = {l:i for i,l in enumerate(labels)}\n",
    "    y_train = [lab2idx[lab] for lab,_ in train_pairs]\n",
    "\n",
    "    # keep only TEST rows whose label is known from train\n",
    "    test_pairs = [(lab, txt) for lab, txt in test_pairs if lab in lab2idx]\n",
    "    y_test = [lab2idx[lab] for lab,_ in test_pairs]\n",
    "\n",
    "    # 3) Tokenize docs (doc-level, unigrams only)\n",
    "    train_docs = [doc_tokens(txt, lang, lowercase) for _, txt in train_pairs]\n",
    "    test_docs  = [doc_tokens(txt,  lang, lowercase) for _, txt in test_pairs]\n",
    "\n",
    "    # 4) Unigram vocab (train-only) and featurize\n",
    "    feat2id = build_unigram_vocab(train_docs, min_df=min_df)\n",
    "    X_train = featurize_unigrams(train_docs, feat2id)\n",
    "    X_test  = featurize_unigrams(test_docs,  feat2id)\n",
    "\n",
    "    # 5) Train NB \n",
    "    C, F = len(labels), len(feat2id)\n",
    "    logpi, logtheta = fit_nb(X_train, y_train, C=C, F=F)\n",
    "    y_pred = predict_nb(X_test, logpi, logtheta)\n",
    "\n",
    "    mets = metrics(y_test, y_pred, C=C)\n",
    "    print(f\"[{\"English\" if lang == \"en\" else \"Hindi\"}-{size}] \"\n",
    "          f\"Acc={mets['accuracy']:.3f}  Prec={mets['precision']:.3f}  \"\n",
    "          f\"Rec={mets['recall']:.3f}  F1={mets['f1']:.3f}\")\n",
    "    return mets\n",
    "\n",
    "print(\"Classification using Naive Bayes:\\n\")\n",
    "for sz in [\"2500\", \"15000\", \"30000\"]:\n",
    "    run_nb(\"en\", sz, lowercase=True, min_df=3)\n",
    "\n",
    "print()\n",
    "\n",
    "for sz in [\"2500\", \"15000\", \"30000\"]:\n",
    "    run_nb(\"hi\", sz, lowercase=True, min_df=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
