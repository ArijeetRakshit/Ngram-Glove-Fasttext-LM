{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0a5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv, re, math, unicodedata, time, json, random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "DATA_DIR = Path(\"csv-datasets\")\n",
    "GLOVE_OUT = Path(\"glove-outcomes\")\n",
    "GLOVE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "LOWERCASE = True\n",
    "EMBEDDING_DIM = 50\n",
    "LEARNING_RATE = 0.05\n",
    "NUM_EPOCHS = 5\n",
    "WINDOW_SIZE = 4\n",
    "XMAX = 100.0\n",
    "ALPHA = 0.75\n",
    "RNG_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfc(s):\n",
    "    return unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "def collapse_ws(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "SPLIT_EN = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "SPLIT_HI = re.compile(r\"(?:ред|\\?|!)\\s+\")\n",
    "\n",
    "def sent_split(text, lang):\n",
    "    rx = SPLIT_EN if lang == \"en\" else SPLIT_HI\n",
    "    return [s for s in rx.split(text) if s.strip()]\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tok_en = RegexpTokenizer(r\"[A-Za-z]+'[A-Za-z]+|[A-Za-z]+|\\d+\")\n",
    "tok_hi = RegexpTokenizer(r\"[\\w]+\")\n",
    "\n",
    "def word_tokens(text, lang):\n",
    "    return tok_en.tokenize(text) if lang == \"en\" else tok_hi.tokenize(text)\n",
    "\n",
    "def normalize_text(text, lowercase=True):\n",
    "    t = collapse_ws(nfc(text))\n",
    "    return t.lower() if lowercase else t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61f927ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_labeled(path: Path):\n",
    "    texts, labels = [], []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for r in rdr:\n",
    "            texts.append(r[\"Text\"])\n",
    "            labels.append(r[\"Categories\"])\n",
    "    return texts, labels\n",
    "\n",
    "def make_lm_sentences(texts, lang, lowercase=True):\n",
    "    out = []\n",
    "    for txt in texts:\n",
    "        t = normalize_text(txt, lowercase)\n",
    "        for s in sent_split(t, lang):\n",
    "            toks = word_tokens(s, lang)\n",
    "            if toks:\n",
    "                out.append(toks)\n",
    "    return out\n",
    "\n",
    "def build_cooccurrence(sentences, window_size=4):\n",
    "    vocab_set = set()\n",
    "    cooc = defaultdict(float)\n",
    "    token_freq = Counter()\n",
    "    for words in sentences:\n",
    "        token_freq.update(words)\n",
    "        n = len(words)\n",
    "        for i in range(n):\n",
    "            wi = words[i]\n",
    "            vocab_set.add(wi)\n",
    "            left = max(0, i - window_size)\n",
    "            right = min(n, i + window_size + 1)\n",
    "            for j in range(left, right):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                wj = words[j]\n",
    "                cooc[(wi, wj)] += 1.0 / abs(i - j)\n",
    "    vocab = sorted(vocab_set)\n",
    "    return vocab, cooc, token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42de7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_weight(x):\n",
    "    return (x / XMAX) ** ALPHA if x < XMAX else 1.0\n",
    "\n",
    "def init_glove_params(vocab, dim=50, seed=RNG_SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    W  = {w: rng.normal(0.0, 0.1, size=dim).astype(np.float64) for w in vocab}\n",
    "    C  = {w: rng.normal(0.0, 0.1, size=dim).astype(np.float64) for w in vocab}\n",
    "    bw = {w: 0.0 for w in vocab}\n",
    "    bc = {w: 0.0 for w in vocab}\n",
    "    return W, C, bw, bc\n",
    "\n",
    "def train_glove(cooc, W, C, bw, bc, lr=LEARNING_RATE, epochs=NUM_EPOCHS, seed=RNG_SEED, verbose=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pairs = list(cooc.items())\n",
    "    for ep in range(1, epochs+1):\n",
    "        rng.shuffle(pairs)\n",
    "        loss = 0.0\n",
    "        for (wi, wj), xij in pairs:\n",
    "            wi_vec = W[wi]; wj_vec = C[wj]\n",
    "            bi = bw[wi];  bj = bc[wj]\n",
    "\n",
    "            pred = float(np.dot(wi_vec, wj_vec) + bi + bj)\n",
    "            diff = pred - math.log(xij)\n",
    "            ww = f_weight(xij)\n",
    "\n",
    "            loss += 0.5 * ww * (diff**2)\n",
    "\n",
    "            g = ww * diff\n",
    "            grad_wi = g * wj_vec\n",
    "            grad_wj = g * wi_vec\n",
    "\n",
    "            W[wi]  -= lr * grad_wi\n",
    "            C[wj]  -= lr * grad_wj\n",
    "            bw[wi] -= lr * g\n",
    "            bc[wj] -= lr * g\n",
    "        if verbose:\n",
    "            print(f\"Epoch={ep}/{epochs}  Loss={loss:.6f}\")\n",
    "    return W, C, bw, bc\n",
    "\n",
    "def final_embeddings(W, C):\n",
    "    keys = list(W.keys())\n",
    "    emb = np.stack([W[k] + C[k] for k in keys])\n",
    "    return keys, emb\n",
    "\n",
    "\n",
    "def save_embeddings(lang, size, vocab, emb):\n",
    "    out_npz = GLOVE_OUT / f\"{lang}_{size}_embeddings.npz\"\n",
    "    out_vocab = GLOVE_OUT / f\"{lang}_{size}_vocab.txt\"\n",
    "    np.savez_compressed(out_npz, vocab=np.array(vocab, dtype=object), emb=emb.astype(np.float32))\n",
    "    with out_vocab.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for w in vocab:\n",
    "            f.write(w + \"\\n\")\n",
    "    return out_npz, out_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a71965fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_lookup(vocab, emb):\n",
    "    w2i = {w:i for i,w in enumerate(vocab)}\n",
    "    return w2i, emb\n",
    "\n",
    "def sentence_embedding(text, lang, w2i, emb, lowercase=True):\n",
    "    toks = []\n",
    "    t = normalize_text(text, lowercase)\n",
    "    for s in sent_split(t, lang):\n",
    "        toks.extend(word_tokens(s, lang))\n",
    "    vecs = [emb[w2i[w]] for w in toks if w in w2i]\n",
    "    if not vecs:\n",
    "        return np.zeros(emb.shape[1], dtype=np.float64)\n",
    "    return np.mean(np.vstack(vecs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2480bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(lang: str, size: str):\n",
    "    sub = \"english\" if lang == \"en\" else \"hindi\"\n",
    "    train_csv = DATA_DIR / sub / f\"{sub}_{size}.csv\"\n",
    "    test_csv  = DATA_DIR / sub / f\"{sub}_test.csv\"\n",
    "    print(f\"{sub}_{size} - {sub}_test\\n\")\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # 1) Load & tokenize train\n",
    "    train_texts, train_labels = read_csv_labeled(train_csv)\n",
    "    train_sents = make_lm_sentences(train_texts, lang, LOWERCASE)\n",
    "    t_tok = time.perf_counter()\n",
    "\n",
    "    # 2) Co-occurrence\n",
    "    vocab, cooc, _ = build_cooccurrence(train_sents, WINDOW_SIZE)\n",
    "    t_co = time.perf_counter()\n",
    "\n",
    "    # 3) Train GloVe\n",
    "    W, C, bw, bc = init_glove_params(vocab, EMBEDDING_DIM, seed=RNG_SEED)\n",
    "    W, C, bw, bc = train_glove(cooc, W, C, bw, bc, lr=LEARNING_RATE, epochs=NUM_EPOCHS, seed=RNG_SEED, verbose=True)\n",
    "    t_tr = time.perf_counter()\n",
    "\n",
    "    # 4) Final Embeddings + save\n",
    "    vocab_list, emb = final_embeddings(W, C)\n",
    "    out_npz, out_vocab = save_embeddings(sub, size, vocab_list, emb)\n",
    "\n",
    "    # 5) Vectorize train/test\n",
    "    test_texts, test_labels = read_csv_labeled(test_csv)\n",
    "    t_feat0 = time.perf_counter()\n",
    "    w2i, emb_mat = build_word_lookup(vocab_list, emb)\n",
    "    X_train = np.vstack([sentence_embedding(txt, lang, w2i, emb_mat, LOWERCASE) for txt in train_texts])\n",
    "    X_test  = np.vstack([sentence_embedding(txt, lang, w2i, emb_mat, LOWERCASE) for txt in test_texts])\n",
    "    t_feat1 = time.perf_counter()\n",
    "\n",
    "    # 6) Train classifier\n",
    "    t_cls0 = time.perf_counter()\n",
    "    clf = LogisticRegression(solver=\"saga\", max_iter=300)\n",
    "    clf.fit(X_train, train_labels)\n",
    "    t_cls1 = time.perf_counter()\n",
    "\n",
    "    # 7) Evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc  = accuracy_score(test_labels, y_pred)\n",
    "    prec = precision_score(test_labels, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec  = recall_score(test_labels, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1   = f1_score(test_labels, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    timings = {\n",
    "        \"tokenize_sec\": t_tok - t0,\n",
    "        \"cooc_sec\": t_co - t_tok,\n",
    "        \"train_sec\": t_tr - t_co,\n",
    "        \"feature_sec\": t_feat1 - t_feat0,\n",
    "        \"clf_train_sec\": t_cls1 - t_cls0,\n",
    "        \"total_sec\": t_cls1 - t0\n",
    "    }\n",
    "    metrics = {\"Acc\": float(acc), \"Prec\": float(prec), \"Recall\": float(rec), \"F1\": float(f1)}\n",
    "\n",
    "    print(f\"[{sub}_{size}] Metrics: {metrics}\")\n",
    "    print(f\"[{sub}_{size}] Timings (s): {timings}\")\n",
    "\n",
    "    # 8) Save run result JSON\n",
    "    res_json = GLOVE_OUT / f\"results_{sub}_{size}.json\"\n",
    "    with res_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"lang\": sub,\n",
    "            \"size\": size,\n",
    "            \"metrics\": metrics,\n",
    "            \"timings\": timings\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved: {res_json}\")\n",
    "    return {\"lang\": sub, \"size\": size, \"metrics\": metrics, \"timings\": timings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab516a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove + Logistic Regression\n",
      "english_2500 - english_test\n",
      "\n",
      "Epoch=1/5  Loss=41307.345871\n",
      "Epoch=2/5  Loss=26360.474205\n",
      "Epoch=3/5  Loss=21333.858986\n",
      "Epoch=4/5  Loss=18789.498185\n",
      "Epoch=5/5  Loss=17096.100725\n",
      "[english_2500] Metrics: {'Acc': 0.2451853050655446, 'Prec': 0.0013579112638496928, 'Recall': 0.0032570167643570704, 'F1': 0.0018813081323930624}\n",
      "[english_2500] Timings (s): {'tokenize_sec': 4.838905300013721, 'cooc_sec': 3.3478284999728203, 'train_sec': 79.30883130000439, 'feature_sec': 7.048829399980605, 'clf_train_sec': 18.19417229993269, 'total_sec': 113.49654329998884}\n",
      "Saved: glove-outcomes\\results_english_2500.json\n",
      "\n",
      "english_15000 - english_test\n",
      "\n",
      "Epoch=1/5  Loss=263028.978827\n",
      "Epoch=2/5  Loss=167110.927128\n",
      "Epoch=3/5  Loss=141259.752241\n",
      "Epoch=4/5  Loss=124376.689225\n",
      "Epoch=5/5  Loss=112610.020840\n",
      "[english_15000] Metrics: {'Acc': 0.28855801909694123, 'Prec': 0.005755672644408116, 'Recall': 0.005124240164740214, 'F1': 0.004200307697232256}\n",
      "[english_15000] Timings (s): {'tokenize_sec': 6.356253499980085, 'cooc_sec': 40.49973339994904, 'train_sec': 700.1900778000709, 'feature_sec': 19.937077199923806, 'clf_train_sec': 453.7241928000003, 'total_sec': 1222.4565364998998}\n",
      "Saved: glove-outcomes\\results_english_15000.json\n",
      "\n",
      "english_30000 - english_test\n",
      "\n",
      "Epoch=1/5  Loss=433807.840787\n",
      "Epoch=2/5  Loss=278748.527267\n",
      "Epoch=3/5  Loss=234492.773634\n",
      "Epoch=4/5  Loss=206316.503951\n",
      "Epoch=5/5  Loss=187714.378798\n",
      "[english_30000] Metrics: {'Acc': 0.3039326751901602, 'Prec': 0.008668330174839731, 'Recall': 0.006146946128604888, 'F1': 0.005516224583697783}\n",
      "[english_30000] Timings (s): {'tokenize_sec': 21.705082700005732, 'cooc_sec': 113.4144369000569, 'train_sec': 3237.940915199928, 'feature_sec': 29.07195699994918, 'clf_train_sec': 2358.3879448000807, 'total_sec': 5762.8641429001}\n",
      "Saved: glove-outcomes\\results_english_30000.json\n",
      "\n",
      "hindi_2500 - hindi_test\n",
      "\n",
      "Epoch=1/5  Loss=10654.262133\n",
      "Epoch=2/5  Loss=5809.004863\n",
      "Epoch=3/5  Loss=4681.420472\n",
      "Epoch=4/5  Loss=4038.237733\n",
      "Epoch=5/5  Loss=3551.766815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Bits\\NLP\\Assignment1\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hindi_2500] Metrics: {'Acc': 0.2243, 'Prec': 0.034512738860322904, 'Recall': 0.030723259457458976, 'F1': 0.023913177034079785}\n",
      "[hindi_2500] Timings (s): {'tokenize_sec': 0.20848449994809926, 'cooc_sec': 0.8926194000523537, 'train_sec': 6.580366399954073, 'feature_sec': 7.547845099936239, 'clf_train_sec': 7.504426900064573, 'total_sec': 23.041300100041553}\n",
      "Saved: glove-outcomes\\results_hindi_2500.json\n",
      "\n",
      "hindi_15000 - hindi_test\n",
      "\n",
      "Epoch=1/5  Loss=25276.139193\n",
      "Epoch=2/5  Loss=13818.812897\n",
      "Epoch=3/5  Loss=11608.385477\n",
      "Epoch=4/5  Loss=9915.649718\n",
      "Epoch=5/5  Loss=8479.549426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Bits\\NLP\\Assignment1\\code\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hindi_15000] Metrics: {'Acc': 0.34103333333333335, 'Prec': 0.05475782613061436, 'Recall': 0.055935698148649196, 'F1': 0.04850995508412934}\n",
      "[hindi_15000] Timings (s): {'tokenize_sec': 1.1657862999709323, 'cooc_sec': 4.970735699986108, 'train_sec': 6.601880800095387, 'feature_sec': 9.478118999977596, 'clf_train_sec': 43.84306139999535, 'total_sec': 66.32599689997733}\n",
      "Saved: glove-outcomes\\results_hindi_15000.json\n",
      "\n",
      "hindi_30000 - hindi_test\n",
      "\n",
      "Epoch=1/5  Loss=36929.438454\n",
      "Epoch=2/5  Loss=20866.370931\n",
      "Epoch=3/5  Loss=17562.649332\n",
      "Epoch=4/5  Loss=14790.620442\n",
      "Epoch=5/5  Loss=12521.217120\n",
      "[hindi_30000] Metrics: {'Acc': 0.41126666666666667, 'Prec': 0.11377371231771151, 'Recall': 0.08375792572382872, 'F1': 0.0845313043199627}\n",
      "[hindi_30000] Timings (s): {'tokenize_sec': 2.177713099983521, 'cooc_sec': 9.093387400032952, 'train_sec': 6.892722499906085, 'feature_sec': 11.295224100002088, 'clf_train_sec': 83.61398270004429, 'total_sec': 113.30818529997487}\n",
      "Saved: glove-outcomes\\results_hindi_30000.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "print(\"Glove + Logistic Regression\")\n",
    "todo = [\n",
    "    (\"en\",\"2500\"), (\"en\",\"15000\"), (\"en\",\"30000\"),\n",
    "    (\"hi\",\"2500\"), (\"hi\",\"15000\"), (\"hi\",\"30000\"),\n",
    "]\n",
    "all_results = []\n",
    "for lang, size in todo:\n",
    "    res = run_single_experiment(lang, size)\n",
    "    print()\n",
    "    all_results.append(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
